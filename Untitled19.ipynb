{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEwngsEX2_cC",
        "outputId": "5b864538-e3ed-4a45-edd0-3e86cc092fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 - Loss 0.6914 - AUC 0.9179\n",
            "Epoch 005 - Loss 0.3606 - AUC 0.9275\n",
            "Epoch 010 - Loss 0.3244 - AUC 0.9213\n",
            "Epoch 015 - Loss 0.2945 - AUC 0.9443\n",
            "Epoch 020 - Loss 0.2329 - AUC 0.9719\n",
            "Epoch 025 - Loss 0.1716 - AUC 0.9872\n",
            "Epoch 030 - Loss 0.1416 - AUC 0.9873\n",
            "Epoch 035 - Loss 0.1164 - AUC 0.9892\n",
            "Epoch 040 - Loss 0.1035 - AUC 0.9904\n",
            "Final AUC on training pairs: 0.9904479091895186\n",
            "\n",
            "--- Inference decisions for example orders ---\n",
            "\n",
            "Order O-001: model_number='MX100', nearest_model='MX100' (edit_dist=0)\n",
            " -> DECISION: ACCEPT (exact model number & exact component set match)\n",
            "\n",
            "Order O-002: model_number='MX100', nearest_model='MX100' (edit_dist=0)\n",
            " -> DECISION: REJECT (model number exists but component sets differ)\n",
            "\n",
            "Order O-003: model_number='MX1X', nearest_model='MX100' (edit_dist=2)\n",
            " Bootstrap (intersection): {'motor', 'frame', 'sensorA'}\n",
            " Query-unique components (scores):\n",
            "   antenna: p=0.482 -> suspect_extra\n",
            " Model-unique components (scores):\n",
            "   battery: p=0.994 -> should_be_included\n",
            "\n",
            "Order O-004: model_number='AXX', nearest_model='AX1' (edit_dist=1)\n",
            " Bootstrap (intersection): {'motor', 'battery', 'frame'}\n",
            " Query-unique components (scores):\n",
            "   shield: p=0.000 -> suspect_extra\n",
            " Model-unique components (scores):\n",
            "   antenna: p=0.479 -> could_be_missing\n",
            "   sensorA: p=0.472 -> could_be_missing\n",
            "\n",
            "Order O-005: model_number='BX9', nearest_model='BX9' (edit_dist=0)\n",
            " -> DECISION: REJECT (model number exists but component sets differ)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# gnn_link_predictor_pipeline.py\n",
        "import random\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def edit_distance(a: str, b: str) -> int:\n",
        "    \"\"\"Simple Levenshtein distance (dynamic programming).\"\"\"\n",
        "    la, lb = len(a), len(b)\n",
        "    dp = [[0] * (lb + 1) for _ in range(la + 1)]\n",
        "    for i in range(la + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(lb + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, la + 1):\n",
        "        for j in range(1, lb + 1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[i][j] = min(dp[i-1][j] + 1,     # deletion\n",
        "                           dp[i][j-1] + 1,     # insertion\n",
        "                           dp[i-1][j-1] + cost) # substitution\n",
        "    return dp[la][lb]\n",
        "\n",
        "def jaccard(a: set, b: set) -> float:\n",
        "    if not a and not b: return 1.0\n",
        "    return len(a & b) / len(a | b)\n",
        "\n",
        "# -------------------------\n",
        "# Synthetic data\n",
        "# -------------------------\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "model_dict = {\n",
        "    \"MX100\": {\"motor\", \"frame\", \"sensorA\", \"battery\"},\n",
        "    \"MX200\": {\"motor\", \"frame\", \"sensorB\", \"battery\", \"shield\"},\n",
        "    \"AX1\":   {\"motor\", \"frame\", \"sensorA\", \"battery\", \"antenna\"},\n",
        "    \"AX2\":   {\"motor\", \"frame\", \"sensorC\", \"battery\"},\n",
        "    \"BX9\":   {\"motor\", \"frame\", \"battery\"}\n",
        "}\n",
        "\n",
        "# Build synthetic historical orders for training (noisy)\n",
        "history_orders = []\n",
        "component_pool = set().union(*model_dict.values()).union({\"cable\",\"extraPart\",\"sensorD\"})\n",
        "model_keys = list(model_dict.keys())\n",
        "for i in range(500):\n",
        "    model = random.choice(model_keys)\n",
        "    base = set(model_dict[model])\n",
        "    # randomly drop or add\n",
        "    if random.random() < 0.12 and len(base) > 1:\n",
        "        base = set(random.sample(list(base), len(base)-1))\n",
        "    if random.random() < 0.12:\n",
        "        base = base.union({random.choice(list(component_pool))})\n",
        "    history_orders.append({\"order_id\": f\"H-{i:03d}\", \"model\": model, \"components\": base})\n",
        "\n",
        "# Build a few example query orders to run inference on later\n",
        "example_orders = [\n",
        "    {\"order_id\": \"O-001\", \"model_number\": \"MX100\", \"components\": {\"motor\", \"frame\", \"sensorA\", \"battery\"}},  # exact -> ACCEPT\n",
        "    {\"order_id\": \"O-002\", \"model_number\": \"MX100\", \"components\": {\"motor\", \"frame\", \"sensorA\"}},  # exact model exists but missing -> REJECT\n",
        "    {\"order_id\": \"O-003\", \"model_number\": \"MX1X\", \"components\": {\"motor\", \"frame\", \"sensorA\", \"antenna\"}},  # non-exact -> bootstrap predict\n",
        "    {\"order_id\": \"O-004\", \"model_number\": \"AXX\", \"components\": {\"motor\", \"frame\", \"battery\", \"shield\"}},  # non-exact -> bootstrap\n",
        "    {\"order_id\": \"O-005\", \"model_number\": \"BX9\",  \"components\": {\"motor\", \"frame\", \"battery\", \"extraPart\"}}   # exact model exists but extra -> REJECT\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# Build HeteroData graph (order nodes and component nodes)\n",
        "# -------------------------\n",
        "# Create index mapping for components and orders\n",
        "all_components = sorted(list(component_pool.union(*[o[\"components\"] for o in history_orders])))\n",
        "comp_to_idx = {c: i for i, c in enumerate(all_components)}\n",
        "\n",
        "order_nodes = []  # will be the historical orders\n",
        "for i, ho in enumerate(history_orders):\n",
        "    order_nodes.append(ho[\"order_id\"])\n",
        "\n",
        "order_to_idx = {oid: i for i, oid in enumerate(order_nodes)}\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# Node features: simple learnable embeddings (indices -> embedding)\n",
        "num_order_nodes = len(order_nodes)\n",
        "num_comp_nodes = len(all_components)\n",
        "order_feat_dim = 32\n",
        "comp_feat_dim = 32\n",
        "\n",
        "# initialize node features as embeddings (learnable)\n",
        "data['order'].x = torch.randn((num_order_nodes, order_feat_dim), dtype=torch.float)\n",
        "data['component'].x = torch.randn((num_comp_nodes, comp_feat_dim), dtype=torch.float)\n",
        "\n",
        "# Build edges: order -> has -> component\n",
        "src_orders = []\n",
        "dst_components = []\n",
        "for ho in history_orders:\n",
        "    oid = ho['order_id']\n",
        "    oidx = order_to_idx[oid]\n",
        "    for c in ho['components']:\n",
        "        if c not in comp_to_idx:\n",
        "            continue\n",
        "        cidx = comp_to_idx[c]\n",
        "        src_orders.append(oidx)\n",
        "        dst_components.append(cidx)\n",
        "\n",
        "data['order', 'has', 'component'].edge_index = torch.tensor([src_orders, dst_components], dtype=torch.long)\n",
        "# add reverse edges for message passing\n",
        "data['component', 'rev_has', 'order'].edge_index = torch.tensor([dst_components, src_orders], dtype=torch.long)\n",
        "\n",
        "# -------------------------\n",
        "# Model: Hetero GNN + link predictor head\n",
        "# -------------------------\n",
        "class HeteroGNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        # two-layer HeteroConv with SAGEConv for each relation\n",
        "        self.conv1 = HeteroConv({\n",
        "            ('order','has','component'): SAGEConv((-1, -1), hidden_channels),\n",
        "            ('component','rev_has','order'): SAGEConv((-1, -1), hidden_channels)\n",
        "        }, aggr='mean')\n",
        "        self.conv2 = HeteroConv({\n",
        "            ('order','has','component'): SAGEConv((-1, -1), out_channels),\n",
        "            ('component','rev_has','order'): SAGEConv((-1, -1), out_channels)\n",
        "        }, aggr='mean')\n",
        "        # small MLP to transform each node-type output into a common embedding dim\n",
        "        self.order_lin = Linear(out_channels, out_channels)\n",
        "        self.comp_lin  = Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
        "        x_dict = {k: F.relu(v) for k,v in x_dict.items()}\n",
        "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
        "        # unify dims per node-type\n",
        "        x_order = self.order_lin(x_dict['order'])\n",
        "        x_comp  = self.comp_lin(x_dict['component'])\n",
        "        return {'order': x_order, 'component': x_comp}\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        # MLP on concatenation of order_emb and comp_emb => probability\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim*2, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(emb_dim, 1)\n",
        "        )\n",
        "    def forward(self, order_emb, comp_emb):\n",
        "        x = torch.cat([order_emb, comp_emb], dim=-1)\n",
        "        return torch.sigmoid(self.mlp(x)).squeeze(-1)\n",
        "\n",
        "# -------------------------\n",
        "# Negative sampling helpers and dataset preparation\n",
        "# -------------------------\n",
        "# Build adjacency set for fast negative sampling\n",
        "adj = defaultdict(set)\n",
        "for s, t in zip(src_orders, dst_components):\n",
        "    adj[s].add(t)\n",
        "\n",
        "# create all positive pairs (order_idx, comp_idx)\n",
        "positive_pairs = [(s, t) for s, t in zip(src_orders, dst_components)]\n",
        "\n",
        "def sample_negative_for_order(o_idx, k=1):\n",
        "    # sample k component indices not connected to order o_idx\n",
        "    negatives = []\n",
        "    while len(negatives) < k:\n",
        "        c = random.randrange(num_comp_nodes)\n",
        "        if c not in adj[o_idx]:\n",
        "            negatives.append(c)\n",
        "    return negatives\n",
        "\n",
        "# Build training triples: for each positive sample, add a negative sample\n",
        "train_pairs = []\n",
        "for (o, c) in positive_pairs:\n",
        "    train_pairs.append((o, c, 1))\n",
        "    negs = sample_negative_for_order(o, k=1)\n",
        "    for nc in negs:\n",
        "        train_pairs.append((o, nc, 0))\n",
        "\n",
        "# Convert to tensors (train on all, small-scale)\n",
        "pairs_tensor = torch.tensor([[o,c,label] for (o,c,label) in train_pairs], dtype=torch.long)\n",
        "\n",
        "# -------------------------\n",
        "# Training loop\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "enc = HeteroGNNEncoder(hidden_channels=64, out_channels=64).to(device)\n",
        "pred = LinkPredictor(emb_dim=64).to(device)\n",
        "\n",
        "opt = torch.optim.Adam(list(enc.parameters()) + list(pred.parameters()), lr=0.01, weight_decay=1e-4)\n",
        "bce_loss = nn.BCELoss()\n",
        "\n",
        "data = data.to(device)\n",
        "\n",
        "# We'll precompute edge_index_dict for conv usage\n",
        "edge_index_dict = {\n",
        "    ('order','has','component'): data['order','has','component'].edge_index,\n",
        "    ('component','rev_has','order'): data['component','rev_has','order'].edge_index\n",
        "}\n",
        "\n",
        "# Flatten features dict into x_dict for forward\n",
        "x_dict = {'order': data['order'].x, 'component': data['component'].x}\n",
        "\n",
        "pairs = pairs_tensor.to(device)\n",
        "\n",
        "def evaluate_model():\n",
        "    enc.eval(); pred.eval()\n",
        "    with torch.no_grad():\n",
        "        emb = enc(x_dict, edge_index_dict)\n",
        "        order_embs = emb['order']\n",
        "        comp_embs = emb['component']\n",
        "        y_true = []\n",
        "        y_score = []\n",
        "        for (o,c,label) in train_pairs:\n",
        "            o = int(o); c = int(c); label = int(label)\n",
        "            y_true.append(label)\n",
        "            score = pred(order_embs[o:o+1], comp_embs[c:c+1]).cpu().item()\n",
        "            y_score.append(score)\n",
        "        auc = roc_auc_score(y_true, y_score)\n",
        "    return auc\n",
        "\n",
        "# Training\n",
        "epochs = 40\n",
        "for epoch in range(1, epochs+1):\n",
        "    enc.train(); pred.train()\n",
        "    opt.zero_grad()\n",
        "    emb = enc(x_dict, edge_index_dict)\n",
        "    order_embs = emb['order']\n",
        "    comp_embs = emb['component']\n",
        "    o_idx = pairs[:,0]\n",
        "    c_idx = pairs[:,1]\n",
        "    labels = pairs[:,2].float().to(device)\n",
        "    o_emb = order_embs[o_idx]\n",
        "    c_emb = comp_embs[c_idx]\n",
        "    scores = pred(o_emb, c_emb)\n",
        "    loss = bce_loss(scores, labels)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if epoch % 5 == 0 or epoch==1:\n",
        "        auc = evaluate_model()\n",
        "        print(f\"Epoch {epoch:03d} - Loss {loss.item():.4f} - AUC {auc:.4f}\")\n",
        "\n",
        "# final evaluation on training-like pairs\n",
        "final_auc = evaluate_model()\n",
        "print(\"Final AUC on training pairs:\", final_auc)\n",
        "\n",
        "# -------------------------\n",
        "# Inference logic: nearest-match with edit distance + exact/non-exact policy\n",
        "# -------------------------\n",
        "# helper to find nearest model by edit distance\n",
        "def nearest_model_by_edit_distance(model_number: str, model_dict: dict):\n",
        "    best = None\n",
        "    best_dist = None\n",
        "    for m in model_dict.keys():\n",
        "        d = edit_distance(model_number, m)\n",
        "        if best is None or d < best_dist:\n",
        "            best = m\n",
        "            best_dist = d\n",
        "    return best, best_dist\n",
        "\n",
        "# link scoring function using trained model (returns probability)\n",
        "def score_link_for_order_component(order_id: str, comp_name: str):\n",
        "    \"\"\"\n",
        "    For inference we may have orders not present in the training set.\n",
        "    For simplicity we will create a temporary order embedding by:\n",
        "     - if the order exists as a training order: use its embedding\n",
        "     - else: create an embedding from average of bootstrap component embeddings or zeros fallback\n",
        "    \"\"\"\n",
        "    enc.eval(); pred.eval()\n",
        "    with torch.no_grad():\n",
        "        emb = enc(x_dict, edge_index_dict)\n",
        "        order_embs = emb['order']\n",
        "        comp_embs = emb['component']\n",
        "\n",
        "        # if order exists in training orders, use its embedding\n",
        "        if order_id in order_to_idx:\n",
        "            oidx = order_to_idx[order_id]\n",
        "            o_vec = order_embs[oidx:oidx+1]\n",
        "        else:\n",
        "            # fallback: zeros vector (could be replaced with learned prior or embedding generated by\n",
        "            # averaging embeddings of contextual components; caller can pass bootstrap context)\n",
        "            o_vec = torch.zeros((1, order_embs.shape[1]), device=device)\n",
        "\n",
        "        if comp_name not in comp_to_idx:\n",
        "            # unknown component -> return low probability\n",
        "            return 0.0\n",
        "        cvec = comp_embs[comp_to_idx[comp_name]: comp_to_idx[comp_name]+1]\n",
        "        return float(pred(o_vec, cvec).cpu().item())\n",
        "\n",
        "# A helper that scores components given a bootstrap set: create a temporary order embedding as mean of bootstrap components embeddings\n",
        "def score_component_given_bootstrap(bootstrap_components: set, candidate: str):\n",
        "    enc.eval(); pred.eval()\n",
        "    with torch.no_grad():\n",
        "        emb = enc(x_dict, edge_index_dict)\n",
        "        comp_embs = emb['component']\n",
        "        if candidate not in comp_to_idx:\n",
        "            return 0.0\n",
        "        cvec = comp_embs[comp_to_idx[candidate]: comp_to_idx[candidate]+1]\n",
        "        if len(bootstrap_components) == 0:\n",
        "            # no context -> use global prior: mean order embedding (mean of all order embeddings)\n",
        "            order_emb_mean = emb['order'].mean(dim=0, keepdim=True)\n",
        "            return float(pred(order_emb_mean, cvec).cpu().item())\n",
        "        else:\n",
        "            idxs = [comp_to_idx[c] for c in bootstrap_components if c in comp_to_idx]\n",
        "            if not idxs:\n",
        "                order_emb_mean = emb['order'].mean(dim=0, keepdim=True)\n",
        "                return float(pred(order_emb_mean, cvec).cpu().item())\n",
        "            mean_vec = comp_embs[idxs].mean(dim=0, keepdim=True)\n",
        "            # Optionally transform mean_vec to order-space; here we directly use as order embedding\n",
        "            return float(pred(mean_vec, cvec).cpu().item())\n",
        "\n",
        "# Now run through example orders applying the decision rules\n",
        "print(\"\\n--- Inference decisions for example orders ---\\n\")\n",
        "for qo in example_orders:\n",
        "    qid = qo['order_id']\n",
        "    q_model = qo['model_number']\n",
        "    q_comps = set(qo['components'])\n",
        "    nearest, dist = nearest_model_by_edit_distance(q_model, model_dict)\n",
        "    model_comps = model_dict[nearest]\n",
        "\n",
        "    print(f\"Order {qid}: model_number='{q_model}', nearest_model='{nearest}' (edit_dist={dist})\")\n",
        "    # exact model-number match path\n",
        "    if q_model in model_dict and q_model == nearest and q_comps == model_comps:\n",
        "        print(\" -> DECISION: ACCEPT (exact model number & exact component set match)\\n\")\n",
        "        continue\n",
        "    if q_model in model_dict and q_model == nearest and q_comps != model_comps:\n",
        "        print(\" -> DECISION: REJECT (model number exists but component sets differ)\\n\")\n",
        "        continue\n",
        "\n",
        "    # non-exact nearest: bootstrap & use GNN link predictor\n",
        "    bootstrap = q_comps & model_comps\n",
        "    query_uniques = q_comps - model_comps\n",
        "    model_uniques = model_comps - q_comps\n",
        "\n",
        "    print(\" Bootstrap (intersection):\", bootstrap)\n",
        "    # score query unique components (are they likely given bootstrap?)\n",
        "    if query_uniques:\n",
        "        print(\" Query-unique components (scores):\")\n",
        "        for c in sorted(query_uniques):\n",
        "            p = score_component_given_bootstrap(bootstrap, c)\n",
        "            flag = \"suspect_extra\" if p < 0.5 else \"ok_extra\"\n",
        "            print(f\"   {c}: p={p:.3f} -> {flag}\")\n",
        "    else:\n",
        "        print(\" Query has no unique components (no extras)\")\n",
        "\n",
        "    # score model unique components (are they likely missing from query?)\n",
        "    if model_uniques:\n",
        "        print(\" Model-unique components (scores):\")\n",
        "        for c in sorted(model_uniques):\n",
        "            p = score_component_given_bootstrap(bootstrap, c)\n",
        "            flag = \"should_be_included\" if p > 0.5 else \"could_be_missing\"\n",
        "            print(f\"   {c}: p={p:.3f} -> {flag}\")\n",
        "    else:\n",
        "        print(\" Model has no unique components (query covers model components)\")\n",
        "\n",
        "    print(\"\")  # newline between orders\n",
        "\n",
        "# End of script"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch_geometric scikit-learn\n",
        "# NOTE: torch_geometric installation often requires following their install instructions:\n",
        "# https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HSi4h_t3YyY",
        "outputId": "b67555a2-197f-4cfe-fec7-7190ebe668f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch_geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch_geometric) (2025.10.5)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extended_gnn_pipeline_with_validation.py\n",
        "# Full script: builds the hetero graph, trains a GNN link predictor, then validates\n",
        "# on synthetic \"good\" and \"bad\" holdout orders producing a confusion matrix.\n",
        "\n",
        "import random\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def edit_distance(a: str, b: str) -> int:\n",
        "    \"\"\"Levenshtein distance (dynamic programming).\"\"\"\n",
        "    la, lb = len(a), len(b)\n",
        "    dp = [[0] * (lb + 1) for _ in range(la + 1)]\n",
        "    for i in range(la + 1):\n",
        "        dp[i][0] = i\n",
        "    for j in range(lb + 1):\n",
        "        dp[0][j] = j\n",
        "    for i in range(1, la + 1):\n",
        "        for j in range(1, lb + 1):\n",
        "            cost = 0 if a[i-1] == b[j-1] else 1\n",
        "            dp[i][j] = min(dp[i-1][j] + 1, dp[i][j-1] + 1, dp[i-1][j-1] + cost)\n",
        "    return dp[la][lb]\n",
        "\n",
        "def jaccard(a: set, b: set) -> float:\n",
        "    if not a and not b: return 1.0\n",
        "    return len(a & b) / len(a | b)\n",
        "\n",
        "# -------------------------\n",
        "# Synthetic data (same as earlier)\n",
        "# -------------------------\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "model_dict = {\n",
        "    \"MX100\": {\"motor\", \"frame\", \"sensorA\", \"battery\"},\n",
        "    \"MX200\": {\"motor\", \"frame\", \"sensorB\", \"battery\", \"shield\"},\n",
        "    \"AX1\":   {\"motor\", \"frame\", \"sensorA\", \"battery\", \"antenna\"},\n",
        "    \"AX2\":   {\"motor\", \"frame\", \"sensorC\", \"battery\"},\n",
        "    \"BX9\":   {\"motor\", \"frame\", \"battery\"}\n",
        "}\n",
        "\n",
        "# Historical orders used for training\n",
        "history_orders = []\n",
        "component_pool = set().union(*model_dict.values()).union({\"cable\",\"extraPart\",\"sensorD\"})\n",
        "model_keys = list(model_dict.keys())\n",
        "for i in range(500):\n",
        "    model = random.choice(model_keys)\n",
        "    base = set(model_dict[model])\n",
        "    # randomly drop or add noise\n",
        "    if random.random() < 0.12 and len(base) > 1:\n",
        "        base = set(random.sample(list(base), len(base)-1))\n",
        "    if random.random() < 0.12:\n",
        "        base = base.union({random.choice(list(component_pool))})\n",
        "    history_orders.append({\"order_id\": f\"H-{i:03d}\", \"model\": model, \"components\": base})\n",
        "\n",
        "# Example queries (for manual inspection later)\n",
        "example_orders = [\n",
        "    {\"order_id\": \"O-001\", \"model_number\": \"MX100\", \"components\": {\"motor\", \"frame\", \"sensorA\", \"battery\"}},  # exact -> ACCEPT\n",
        "    {\"order_id\": \"O-002\", \"model_number\": \"MX100\", \"components\": {\"motor\", \"frame\", \"sensorA\"}},  # exact model exists but missing -> REJECT\n",
        "    {\"order_id\": \"O-003\", \"model_number\": \"MX1X\", \"components\": {\"motor\", \"frame\", \"sensorA\", \"antenna\"}},  # non-exact -> bootstrap predict\n",
        "    {\"order_id\": \"O-004\", \"model_number\": \"AXX\", \"components\": {\"motor\", \"frame\", \"battery\", \"shield\"}},  # non-exact -> bootstrap\n",
        "    {\"order_id\": \"O-005\", \"model_number\": \"BX9\",  \"components\": {\"motor\", \"frame\", \"battery\", \"extraPart\"}}   # exact model exists but extra -> REJECT\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# Build HeteroData graph (order nodes and component nodes)\n",
        "# -------------------------\n",
        "all_components = sorted(list(component_pool.union(*[o[\"components\"] for o in history_orders])))\n",
        "comp_to_idx = {c: i for i, c in enumerate(all_components)}\n",
        "\n",
        "order_nodes = []\n",
        "for i, ho in enumerate(history_orders):\n",
        "    order_nodes.append(ho[\"order_id\"])\n",
        "order_to_idx = {oid: i for i, oid in enumerate(order_nodes)}\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "# Node features: simple seed embeddings (learnable)\n",
        "num_order_nodes = len(order_nodes)\n",
        "num_comp_nodes = len(all_components)\n",
        "order_feat_dim = 32\n",
        "comp_feat_dim = 32\n",
        "\n",
        "data['order'].x = torch.randn((num_order_nodes, order_feat_dim), dtype=torch.float)\n",
        "data['component'].x = torch.randn((num_comp_nodes, comp_feat_dim), dtype=torch.float)\n",
        "\n",
        "# Build order->component edges\n",
        "src_orders = []\n",
        "dst_components = []\n",
        "for ho in history_orders:\n",
        "    oidx = order_to_idx[ho['order_id']]\n",
        "    for c in ho['components']:\n",
        "        if c not in comp_to_idx: continue\n",
        "        cidx = comp_to_idx[c]\n",
        "        src_orders.append(oidx)\n",
        "        dst_components.append(cidx)\n",
        "\n",
        "data['order', 'has', 'component'].edge_index = torch.tensor([src_orders, dst_components], dtype=torch.long)\n",
        "data['component', 'rev_has', 'order'].edge_index = torch.tensor([dst_components, src_orders], dtype=torch.long)\n",
        "\n",
        "# -------------------------\n",
        "# Hetero GNN + Link Predictor\n",
        "# -------------------------\n",
        "class HeteroGNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = HeteroConv({\n",
        "            ('order','has','component'): SAGEConv((-1, -1), hidden_channels),\n",
        "            ('component','rev_has','order'): SAGEConv((-1, -1), hidden_channels)\n",
        "        }, aggr='mean')\n",
        "        self.conv2 = HeteroConv({\n",
        "            ('order','has','component'): SAGEConv((-1, -1), out_channels),\n",
        "            ('component','rev_has','order'): SAGEConv((-1, -1), out_channels)\n",
        "        }, aggr='mean')\n",
        "        self.order_lin = Linear(out_channels, out_channels)\n",
        "        self.comp_lin  = Linear(out_channels, out_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        x = self.conv1(x_dict, edge_index_dict)\n",
        "        x = {k: F.relu(v) for k,v in x.items()}\n",
        "        x = self.conv2(x, edge_index_dict)\n",
        "        x_order = self.order_lin(x['order'])\n",
        "        x_comp  = self.comp_lin(x['component'])\n",
        "        return {'order': x_order, 'component': x_comp}\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim*2, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(emb_dim, 1)\n",
        "        )\n",
        "    def forward(self, order_emb, comp_emb):\n",
        "        x = torch.cat([order_emb, comp_emb], dim=-1)\n",
        "        return torch.sigmoid(self.mlp(x)).squeeze(-1)\n",
        "\n",
        "# -------------------------\n",
        "# Prepare training pairs (positive + negative)\n",
        "# -------------------------\n",
        "adj = defaultdict(set)\n",
        "for s, t in zip(src_orders, dst_components):\n",
        "    adj[s].add(t)\n",
        "\n",
        "positive_pairs = [(s, t) for s, t in zip(src_orders, dst_components)]\n",
        "\n",
        "def sample_negative_for_order(o_idx, k=1):\n",
        "    negatives = []\n",
        "    while len(negatives) < k:\n",
        "        c = random.randrange(num_comp_nodes)\n",
        "        if c not in adj[o_idx]:\n",
        "            negatives.append(c)\n",
        "    return negatives\n",
        "\n",
        "train_pairs = []\n",
        "for (o, c) in positive_pairs:\n",
        "    train_pairs.append((o, c, 1))\n",
        "    negs = sample_negative_for_order(o, k=1)\n",
        "    for nc in negs:\n",
        "        train_pairs.append((o, nc, 0))\n",
        "\n",
        "pairs_tensor = torch.tensor([[o,c,label] for (o,c,label) in train_pairs], dtype=torch.long)\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "enc = HeteroGNNEncoder(hidden_channels=64, out_channels=64).to(device)\n",
        "pred = LinkPredictor(emb_dim=64).to(device)\n",
        "\n",
        "opt = torch.optim.Adam(list(enc.parameters()) + list(pred.parameters()), lr=0.01, weight_decay=1e-4)\n",
        "bce_loss = nn.BCELoss()\n",
        "\n",
        "data = data.to(device)\n",
        "edge_index_dict = {\n",
        "    ('order','has','component'): data['order','has','component'].edge_index,\n",
        "    ('component','rev_has','order'): data['component','rev_has','order'].edge_index\n",
        "}\n",
        "x_dict = {'order': data['order'].x, 'component': data['component'].x}\n",
        "pairs = pairs_tensor.to(device)\n",
        "\n",
        "def evaluate_model_auc(pairs_list):\n",
        "    enc.eval(); pred.eval()\n",
        "    with torch.no_grad():\n",
        "        emb = enc(x_dict, edge_index_dict)\n",
        "        order_embs = emb['order']\n",
        "        comp_embs = emb['component']\n",
        "        y_true = []\n",
        "        y_score = []\n",
        "        for (o,c,label) in pairs_list:\n",
        "            o = int(o); c = int(c); label = int(label)\n",
        "            y_true.append(label)\n",
        "            score = pred(order_embs[o:o+1], comp_embs[c:c+1]).cpu().item()\n",
        "            y_score.append(score)\n",
        "        return roc_auc_score(y_true, y_score)\n",
        "\n",
        "epochs = 40\n",
        "for epoch in range(1, epochs+1):\n",
        "    enc.train(); pred.train()\n",
        "    opt.zero_grad()\n",
        "    emb = enc(x_dict, edge_index_dict)\n",
        "    order_embs = emb['order']\n",
        "    comp_embs = emb['component']\n",
        "    o_idx = pairs[:,0]\n",
        "    c_idx = pairs[:,1]\n",
        "    labels = pairs[:,2].float().to(device)\n",
        "    o_emb = order_embs[o_idx]\n",
        "    c_emb = comp_embs[c_idx]\n",
        "    scores = pred(o_emb, c_emb)\n",
        "    loss = bce_loss(scores, labels)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if epoch % 5 == 0 or epoch==1:\n",
        "        auc = evaluate_model_auc(train_pairs)\n",
        "        print(f\"Epoch {epoch:03d} - Loss {loss.item():.4f} - AUC {auc:.4f}\")\n",
        "\n",
        "final_auc = evaluate_model_auc(train_pairs)\n",
        "print(\"Final AUC on training-like pairs:\", final_auc)\n",
        "\n",
        "# -------------------------\n",
        "# Inference helper functions (reuse encoder & predictor)\n",
        "# -------------------------\n",
        "def nearest_model_by_edit_distance(model_number: str, model_dict: dict):\n",
        "    best = None\n",
        "    best_dist = None\n",
        "    for m in model_dict.keys():\n",
        "        d = edit_distance(model_number, m)\n",
        "        if best is None or d < best_dist:\n",
        "            best = m\n",
        "            best_dist = d\n",
        "    return best, best_dist\n",
        "\n",
        "# Score candidate component given bootstrap context using trained models\n",
        "def score_component_given_bootstrap(bootstrap_components: set, candidate: str, enc_model, pred_model, x_dict_local, edge_index_local, comp_to_idx_local):\n",
        "    enc_model.eval(); pred_model.eval()\n",
        "    with torch.no_grad():\n",
        "        emb = enc_model(x_dict_local, edge_index_local)\n",
        "        comp_embs = emb['component']\n",
        "        if candidate not in comp_to_idx_local:\n",
        "            return 0.0\n",
        "        cvec = comp_embs[comp_to_idx_local[candidate]: comp_to_idx_local[candidate]+1]\n",
        "        if len(bootstrap_components) == 0:\n",
        "            order_emb_mean = emb['order'].mean(dim=0, keepdim=True)\n",
        "            return float(pred_model(order_emb_mean, cvec).cpu().item())\n",
        "        else:\n",
        "            idxs = [comp_to_idx_local[c] for c in bootstrap_components if c in comp_to_idx_local]\n",
        "            if not idxs:\n",
        "                order_emb_mean = emb['order'].mean(dim=0, keepdim=True)\n",
        "                return float(pred_model(order_emb_mean, cvec).cpu().item())\n",
        "            mean_vec = comp_embs[idxs].mean(dim=0, keepdim=True)\n",
        "            # use mean_vec as a proxy order embedding\n",
        "            return float(pred_model(mean_vec, cvec).cpu().item())\n",
        "\n",
        "# Decision function: returns True if accept, False if reject\n",
        "def decide_accept_or_reject(query_order, model_dict, comp_to_idx_local, enc_model, pred_model, x_dict_local, edge_index_local, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Query order: dict with keys 'order_id', 'model_number', 'components'\n",
        "    Returns: (accept_bool, reason_dict)\n",
        "    \"\"\"\n",
        "    qid = query_order['order_id']\n",
        "    qmodel = query_order['model_number']\n",
        "    qcomps = set(query_order['components'])\n",
        "    nearest, dist = nearest_model_by_edit_distance(qmodel, model_dict)\n",
        "    model_comps = model_dict[nearest]\n",
        "\n",
        "    # Exact model-number exists path\n",
        "    if qmodel in model_dict and qmodel == nearest:\n",
        "        if qcomps == model_comps:\n",
        "            return True, {\"route\": \"exact_match_components_equal\", \"nearest\": nearest, \"dist\": dist}\n",
        "        else:\n",
        "            return False, {\"route\": \"exact_model_components_differ\", \"nearest\": nearest, \"dist\": dist,\n",
        "                           \"model_components\": model_comps}\n",
        "\n",
        "    # Non-exact: bootstrap + link predictor scoring\n",
        "    bootstrap = qcomps & model_comps\n",
        "    query_uniques = qcomps - model_comps\n",
        "    model_uniques = model_comps - qcomps\n",
        "\n",
        "    # If bootstrap is empty we still proceed; scores will rely on global priors\n",
        "    # Check query-unique components: if any is unlikely given bootstrap -> reject (extra)\n",
        "    for c in query_uniques:\n",
        "        p = score_component_given_bootstrap(bootstrap, c, enc_model, pred_model, x_dict_local, edge_index_local, comp_to_idx_local)\n",
        "        if p < threshold:\n",
        "            return False, {\"route\": \"reject_extra\", \"nearest\": nearest, \"dist\": dist, \"bootstrap\": bootstrap, \"offending_component\": c, \"prob\": p}\n",
        "\n",
        "    # Check model-unique components: if any is likely given bootstrap -> reject (missing)\n",
        "    for c in model_uniques:\n",
        "        p = score_component_given_bootstrap(bootstrap, c, enc_model, pred_model, x_dict_local, edge_index_local, comp_to_idx_local)\n",
        "        if p > threshold:\n",
        "            return False, {\"route\": \"reject_missing\", \"nearest\": nearest, \"dist\": dist, \"bootstrap\": bootstrap, \"missing_component\": c, \"prob\": p}\n",
        "\n",
        "    # If passed both checks -> accept\n",
        "    return True, {\"route\": \"non_exact_accept\", \"nearest\": nearest, \"dist\": dist, \"bootstrap\": bootstrap}\n",
        "\n",
        "# -------------------------\n",
        "# Generate holdout \"good\" and \"bad\" orders\n",
        "# -------------------------\n",
        "# Good orders: should be accepted by policy. Create:\n",
        "#  - exact matches for some models\n",
        "#  - near variants that include plausible optional parts (co-occurring)\n",
        "# Bad orders:\n",
        "#  - missing core component(s)\n",
        "#  - include unlikely extras (random extras that don't co-occur)\n",
        "good_holdout = []\n",
        "bad_holdout = []\n",
        "\n",
        "# 1) exact-good: exact component sets\n",
        "for i, m in enumerate(model_keys):\n",
        "    good_holdout.append({\"order_id\": f\"G-exact-{i}\", \"model_number\": m, \"components\": set(model_dict[m])})\n",
        "\n",
        "# 2) plausible-variant-good: bootstrap will support optional parts (we create extras that co-occur often)\n",
        "# Build frequency co-occurrence from training history to pick plausible extras\n",
        "cooc_counts = defaultdict(int)\n",
        "for ho in history_orders:\n",
        "    comps = list(ho['components'])\n",
        "    for a in comps:\n",
        "        for b in comps:\n",
        "            if a != b:\n",
        "                cooc_counts[(a,b)] += 1\n",
        "\n",
        "# For each model, pick a common co-occurring part (if exists) and add to make it still \"good\"\n",
        "for i, m in enumerate(model_keys):\n",
        "    base = set(model_dict[m])\n",
        "    # find candidate extra with high co-occurrence with many base parts\n",
        "    candidates = []\n",
        "    for cand in all_components:\n",
        "        if cand in base: continue\n",
        "        # score = sum cooc with base parts\n",
        "        score = sum(cooc_counts.get((b, cand), 0) for b in base)\n",
        "        if score > 0:\n",
        "            candidates.append((score, cand))\n",
        "    if candidates:\n",
        "        candidates.sort(reverse=True)\n",
        "        chosen = candidates[0][1]\n",
        "        # create a good order variant that includes plausible extra\n",
        "        good_holdout.append({\"order_id\": f\"G-variant-{i}\", \"model_number\": m + \"X\", \"components\": base.union({chosen})})\n",
        "\n",
        "# 3) bad orders: missing required components (drop core parts)\n",
        "for i, m in enumerate(model_keys):\n",
        "    base = set(model_dict[m])\n",
        "    # drop a core component (prefer non-optional: here just drop element)\n",
        "    if len(base) > 1:\n",
        "        dropped = random.choice(list(base))\n",
        "        bad_holdout.append({\"order_id\": f\"B-missing-{i}\", \"model_number\": m + \"BAD\", \"components\": base - {dropped}})\n",
        "\n",
        "# 4) bad orders: add unlikely extras (random), pick parts with low co-occurrence\n",
        "rare_candidates = [c for c in all_components if all(cooc_counts.get((b,c),0) < 3 for b in all_components)]\n",
        "for i in range(len(model_keys)):\n",
        "    m = random.choice(model_keys)\n",
        "    base = set(model_dict[m])\n",
        "    rare = random.choice(rare_candidates) if rare_candidates else \"extraPart\"\n",
        "    bad_holdout.append({\"order_id\": f\"B-extra-{i}\", \"model_number\": m + \"BADX\", \"components\": base.union({rare})})\n",
        "\n",
        "# Ensure holdouts are reasonably sized\n",
        "print(\"Generated holdout sizes:\", len(good_holdout), \"good |\", len(bad_holdout), \"bad\")\n",
        "\n",
        "# -------------------------\n",
        "# Validation: run decision for each holdout and produce confusion matrix\n",
        "# -------------------------\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Threshold for link predictor decisions\n",
        "threshold = 0.5\n",
        "\n",
        "# Evaluate good_holdout: expected label = 1 (good)\n",
        "for qo in good_holdout:\n",
        "    accept, info = decide_accept_or_reject(qo, model_dict, comp_to_idx, enc, pred, x_dict, edge_index_dict, threshold=threshold)\n",
        "    y_true.append(1)\n",
        "    y_pred.append(1 if accept else 0)\n",
        "    # optional: you can print per-order info for debugging\n",
        "    # print(\"GOOD\", qo['order_id'], \"->\", \"ACCEPT\" if accept else \"REJECT\", info)\n",
        "\n",
        "# Evaluate bad_holdout: expected label = 0 (bad)\n",
        "for qo in bad_holdout:\n",
        "    accept, info = decide_accept_or_reject(qo, model_dict, comp_to_idx, enc, pred, x_dict, edge_index_dict, threshold=threshold)\n",
        "    y_true.append(0)\n",
        "    y_pred.append(1 if accept else 0)\n",
        "    # optional debug print\n",
        "    # print(\"BAD \", qo['order_id'], \"->\", \"ACCEPT\" if accept else \"REJECT\", info)\n",
        "\n",
        "# Compute confusion matrix: order of labels default (0,1) -> we'll map to standard\n",
        "tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0,1]).ravel()\n",
        "print(\"\\nConfusion matrix counts:\")\n",
        "print(f\"TP (accept good): {tp}\")\n",
        "print(f\"FN (reject good): {fn}\")\n",
        "print(f\"TN (reject bad): {tn}\")\n",
        "print(f\"FP (accept bad): {fp}\")\n",
        "\n",
        "# Print simple metrics\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "print(f\"\\nValidation metrics: precision={precision:.3f}, recall={recall:.3f}, accuracy={accuracy:.3f}\")\n",
        "\n",
        "# Optionally, show per-case decisions for inspection\n",
        "print(\"\\nDetailed decisions for holdouts (first 10 shown):\")\n",
        "combined = [(\"GOOD\", qo) for qo in good_holdout] + [(\"BAD\", qo) for qo in bad_holdout]\n",
        "for label, qo in combined[:10]:\n",
        "    accept, info = decide_accept_or_reject(qo, model_dict, comp_to_idx, enc, pred, x_dict, edge_index_dict, threshold=threshold)\n",
        "    print(f\"{label} {qo['order_id']} -> {'ACCEPT' if accept else 'REJECT'} via {info['route']} (nearest={info.get('nearest')}, dist={info.get('dist')})\")\n",
        "\n",
        "# -------------------------\n",
        "# Notes about training paradigm updates for this application\n",
        "# -------------------------\n",
        "notes = \"\"\"\n",
        "Notes / Recommended updates to training paradigm for better validation fidelity:\n",
        "\n",
        "1) Context sampling during training:\n",
        "   - During training we must simulate the exact inference 'bootstrap' situation: randomly sub-sample each training order's components\n",
        "     to create partial contexts and train the link predictor to predict missing/extra components given the partial context. This was\n",
        "     approximated by sampling modified historical orders earlier, but you can make it explicit: for each positive example, sample a\n",
        "     bootstrap subset B subset-of C_order and use (B, candidate) as training context.\n",
        "\n",
        "2) Hard-negative mining:\n",
        "   - For validation, the model must handle subtle plausible negatives (e.g., components that often co-occur but are wrong for this variant).\n",
        "     Hard-negative mining will improve discrimination.\n",
        "\n",
        "3) Threshold calibration:\n",
        "   - We used 0.5 as default. In production calibrate threshold on a validation set (ROC/Precision-Recall or cost-sensitive loss depending on FP/FN costs).\n",
        "\n",
        "4) Deterministic features & order encoder:\n",
        "   - Replace random node initializations with deterministic features (one-hot model family, product type, tokenized model number embeddings).\n",
        "     Also add an order-encoder (MLP that consumes a binary bag-of-components or component embeddings) to create consistent inference-time\n",
        "     order embeddings from bootstrap components.\n",
        "\n",
        "5) Larger/representative holdout:\n",
        "   - Validate across multiple families, time-splits (simulate new model numbers), and noisy patterns present in production.\n",
        "\n",
        "6) Logging and human-in-the-loop:\n",
        "   - For ambiguous cases (probability near threshold or tiny bootstrap), route to review and collect labelled corrections to improve training data.\n",
        "\"\"\"\n",
        "print(notes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTYBA3pwgHSI",
        "outputId": "bfbf5b62-4087-42c0-9d09-d7a230beea3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 - Loss 0.6920 - AUC 0.9212\n",
            "Epoch 005 - Loss 0.3649 - AUC 0.9285\n",
            "Epoch 010 - Loss 0.3114 - AUC 0.9240\n",
            "Epoch 015 - Loss 0.2828 - AUC 0.9446\n",
            "Epoch 020 - Loss 0.2121 - AUC 0.9773\n",
            "Epoch 025 - Loss 0.1635 - AUC 0.9863\n",
            "Epoch 030 - Loss 0.1372 - AUC 0.9860\n",
            "Epoch 035 - Loss 0.1171 - AUC 0.9886\n",
            "Epoch 040 - Loss 0.1192 - AUC 0.9895\n",
            "Final AUC on training-like pairs: 0.9894556508624753\n",
            "Generated holdout sizes: 10 good | 10 bad\n",
            "\n",
            "Confusion matrix counts:\n",
            "TP (accept good): 8\n",
            "FN (reject good): 2\n",
            "TN (reject bad): 9\n",
            "FP (accept bad): 1\n",
            "\n",
            "Validation metrics: precision=0.889, recall=0.800, accuracy=0.850\n",
            "\n",
            "Detailed decisions for holdouts (first 10 shown):\n",
            "GOOD G-exact-0 -> ACCEPT via exact_match_components_equal (nearest=MX100, dist=0)\n",
            "GOOD G-exact-1 -> ACCEPT via exact_match_components_equal (nearest=MX200, dist=0)\n",
            "GOOD G-exact-2 -> ACCEPT via exact_match_components_equal (nearest=AX1, dist=0)\n",
            "GOOD G-exact-3 -> ACCEPT via exact_match_components_equal (nearest=AX2, dist=0)\n",
            "GOOD G-exact-4 -> ACCEPT via exact_match_components_equal (nearest=BX9, dist=0)\n",
            "GOOD G-variant-0 -> REJECT via reject_extra (nearest=MX100, dist=1)\n",
            "GOOD G-variant-1 -> ACCEPT via non_exact_accept (nearest=MX200, dist=1)\n",
            "GOOD G-variant-2 -> REJECT via reject_extra (nearest=AX1, dist=1)\n",
            "GOOD G-variant-3 -> ACCEPT via non_exact_accept (nearest=AX2, dist=1)\n",
            "GOOD G-variant-4 -> ACCEPT via non_exact_accept (nearest=BX9, dist=1)\n",
            "\n",
            "Notes / Recommended updates to training paradigm for better validation fidelity:\n",
            "\n",
            "1) Context sampling during training:\n",
            "   - During training we must simulate the exact inference 'bootstrap' situation: randomly sub-sample each training order's components\n",
            "     to create partial contexts and train the link predictor to predict missing/extra components given the partial context. This was\n",
            "     approximated by sampling modified historical orders earlier, but you can make it explicit: for each positive example, sample a\n",
            "     bootstrap subset B subset-of C_order and use (B, candidate) as training context.\n",
            "\n",
            "2) Hard-negative mining:\n",
            "   - For validation, the model must handle subtle plausible negatives (e.g., components that often co-occur but are wrong for this variant).\n",
            "     Hard-negative mining will improve discrimination.\n",
            "\n",
            "3) Threshold calibration:\n",
            "   - We used 0.5 as default. In production calibrate threshold on a validation set (ROC/Precision-Recall or cost-sensitive loss depending on FP/FN costs).\n",
            "\n",
            "4) Deterministic features & order encoder:\n",
            "   - Replace random node initializations with deterministic features (one-hot model family, product type, tokenized model number embeddings).\n",
            "     Also add an order-encoder (MLP that consumes a binary bag-of-components or component embeddings) to create consistent inference-time\n",
            "     order embeddings from bootstrap components.\n",
            "\n",
            "5) Larger/representative holdout:\n",
            "   - Validate across multiple families, time-splits (simulate new model numbers), and noisy patterns present in production.\n",
            "\n",
            "6) Logging and human-in-the-loop:\n",
            "   - For ambiguous cases (probability near threshold or tiny bootstrap), route to review and collect labelled corrections to improve training data.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}